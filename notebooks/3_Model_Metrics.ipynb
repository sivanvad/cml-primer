{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b4bf3b-4be4-4a64-bfe8-7acd8096ccdc",
   "metadata": {},
   "source": [
    "# Model Metrics - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a894b-c409-4813-810d-b862239b5129",
   "metadata": {},
   "source": [
    "When a model is generated, it may or may not have 'good' score. Models can score high on training dataset but perform worse on validation dataset. In either case, hyperparameter tuning plays an important role in tweaking the model to avoid overfitting and underfitting. Generalised models perform consistently on any dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35b082-6522-4a9b-be6b-45f554067e55",
   "metadata": {},
   "source": [
    "CML Experiments feature provides an mechanism for model tuning with different dataset, features and algorithms. Using Experiments and model metrics, hyperparameter tuning is automated and expedited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29689d72-957b-4adc-afb3-0eec12fe5014",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cd3af-6762-40da-ab39-d3af757c5fdf",
   "metadata": {},
   "source": [
    "First step is to create a script that takes range of parameters, trains & scores the model and provide an output of best set of parameters. Metrics generated in an experiment run are tracked by MLFlow and logged in CML. While this can also be achieved using Jupyter notebook, we will create python script for reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60268833-a2ab-4e1b-9141-a9453185149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6d6073-63c8-46d1-b2d9-a90145231ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training dataset\n",
    "pd_df = pd.read_csv('/home/cdsw/data/UCI_Credit_Card.csv.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8405da-684c-4f21-9b5a-e57cd54e4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['SEX', 'EDUCATION', 'MARRIAGE', 'AGE_GROUP']\n",
    "numeric_cols = list(set(pd_df.columns) - set(cat_cols) - set(['ID', 'default.payment.next.month', 'AGE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7abab79-69ed-4a62-87ca-d7ff3354670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing\n",
    "pd_df = pd_df.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34d0012-4925-4188-ba5b-6dfaff35df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd_df.replace({'EDUCATION': {0 : 5, 6: 5}})\n",
    "df_clean = df_clean.replace({'MARRIAGE' : {0: 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b088a2ea-93f3-4a12-80b8-c4a56b954561",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_late_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5' , 'PAY_6']\n",
    "for col in pay_late_cols:\n",
    "    df_clean.loc[df_clean[col] < 0, col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa48a27-6935-4fb2-bb31-4c74c129d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bill_ratio(df):\n",
    "    blr_cols = []\n",
    "    bins= [20, 30, 40, 50, 60, 100]\n",
    "    labels = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    for mth in range(1, 7, 1):\n",
    "        blr_cols.append(['BLR'+str(mth), 'BILL_AMT'+str(mth)])\n",
    "    \n",
    "    for col_blr in blr_cols:\n",
    "        df[col_blr[0]] = df[col_blr[1]] / df_clean['LIMIT_BAL']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41b79a08-161d-4dee-84ac-e801c9b8e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_group(df):\n",
    "    bins= [20, 30, 40, 50, 60, 100]\n",
    "    labels = [1, 2, 3, 4, 5]\n",
    "    df['AGE_GROUP'] = pd.cut(df['AGE'], bins=bins, labels=labels, right=False)\n",
    "    df = df.drop('AGE', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2ba71c-f143-4376-a409-3431f11bb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "df_ftr_1 = get_bill_ratio(df_clean)\n",
    "df_ftr = get_age_group(df_ftr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aa09711-6a53-4074-92d6-3f6bc2343f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_ftr.drop('default.payment.next.month', axis=1).copy()\n",
    "y = df_ftr['default.payment.next.month'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b798af-48f4-4cee-bcea-43c4c47c82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698c74e-1065-4519-8f55-91a604dd73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameter tuning\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "scaler = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "one_hot_encoder = Pipeline(\n",
    "    steps=[('encoder', OneHotEncoder())]\n",
    ")\n",
    "\n",
    "rf_clf = RandomForestClassifier(class_weight = \"balanced\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", scaler, numeric_cols),\n",
    "        (\"cat\", one_hot_encoder, cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", rf_clf)]\n",
    ")\n",
    "\n",
    "parameters = {'classifier__n_estimators': [100, 150, 200, 250, 300], 'classifier__max_depth': list(range(1, 20, 5)), \n",
    "              'classifier__min_samples_split': np.arange(0.1, 1.0, 0.5), 'classifier__min_samples_leaf': np.arange(0.1, 0.5, 0.2)}\n",
    "\n",
    "optimizer = GridSearchCV(clf, parameters)\n",
    "\n",
    "model = optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61661bfe-bf3a-4458-9c43-77f6e8815c28",
   "metadata": {},
   "source": [
    "### MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e295f1-2c93-475b-b061-d6044a090bb9",
   "metadata": {},
   "source": [
    "MLFlow is embedded into CML experiments feature for tracking. Using MLFlow, experiments can be started and metrics & params are tracked for each run. This helps log metrics and compare run results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21fab5-3817-4b0a-b96b-7f00457f0341",
   "metadata": {},
   "source": [
    "Create an experiment from Experiments page in CML project. After creating the experiment, you can then reference the experiment name to track each run of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490cad1-6496-40b9-a453-9ef57a4492ea",
   "metadata": {},
   "source": [
    "<img src=\"../docs/Experiments.png\" width=\"625\" height=\"1250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab7535-d092-4e0b-9e16-e752dc0c73ff",
   "metadata": {},
   "source": [
    "Below is an example of commands that can be used for tracking metrics and params from script created earlier:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95b9b563-6237-4c75-922a-a22e8978c4f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.set_experiment('uci_creditcard_exp2')\n",
    "mlflow.start_run()\n",
    "\n",
    "mlflow.log_metric('best_score', best_score)\"\n",
    "mlflow.log_param('max_depth', optimal_max_depth)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb6df1-2c64-4908-90e0-5cb3f519d0ff",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637bb56-1dee-41b9-b4d5-86178c4e100e",
   "metadata": {},
   "source": [
    "Putting all together, [hyperparameter_tuning.py](../scripts/hyperparameter_tuning.py) is created. Run the script and go to Experiments page to monitor the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a0dbb56-30ff-46cc-804d-fd8a20b997c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'testmodel'.\n",
      "2024/03/15 00:56:32 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: testmodel, version 1\n",
      "Created version '1' of model 'testmodel'.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/3_hyperparameter_tuning.py uci_creditcard_exp0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51677980-4ac4-4b9d-8d1b-aaa63073305e",
   "metadata": {},
   "source": [
    "For every run, metrics are logged. After running the experiment, navigate to experiment page and look at the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232aa5e-125e-4f79-9a6a-861181dad38b",
   "metadata": {},
   "source": [
    "<img src=\"../docs/Experiment_Runs.png\" width=\"725\" height=\"1250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e9756-d9ab-4e7f-8c4e-0b720f12ec87",
   "metadata": {},
   "source": [
    "Once the runs complete, the metrics and parameters can be compared between runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eeca4d-e851-4463-b4ea-b903d16c63ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../docs/Experiment_Compare_Runs.png\" width=\"725\" height=\"1250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b4eb6-c70d-4458-b3a5-8448a720e2ab",
   "metadata": {},
   "source": [
    "It can be noted that the best performing parameters are logged along with their score. This can then be used for future training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
